"""
Interface utilisateur Streamlit pour tester le mode "un par un" de Fluxgym-coach.
Permet de charger une image, g√©n√©rer une description, et lancer l'am√©lioration.
"""

import streamlit as st
from pathlib import Path
from PIL import Image
import sys
import os
import torch
import time
import hashlib
import shutil
import io
from datetime import datetime
from transformers import AutoProcessor, BlipForConditionalGeneration

# Configuration des dossiers
BASE_DIR = Path(__file__).parent.absolute()
TEMP_DIR = BASE_DIR / 'temp_processing'
TEMP_DIR.mkdir(exist_ok=True)

def clean_temp_dir():
    """Vide le dossier temporaire."""
    for item in TEMP_DIR.glob('*'):
        try:
            if item.is_file():
                item.unlink()
            elif item.is_dir():
                shutil.rmtree(item)
        except Exception as e:
            print(f"Erreur lors de la suppression de {item}: {e}")

def process_uploaded_image(uploaded_file):
    """
    Traite le fichier upload√© :
    1. Calcule son hash
    2. Cr√©e un nom de fichier unique avec le hash
    3. Convertit en PNG si n√©cessaire
    4. Enregistre dans le dossier temporaire
    
    Retourne le chemin du fichier trait√©
    """
    # Nettoyer d'abord le dossier temporaire
    clean_temp_dir()
    
    # Lire le contenu du fichier
    file_bytes = uploaded_file.getvalue()
    
    # Calculer le hash du fichier
    file_hash = hashlib.sha256(file_bytes).hexdigest()
    
    # Cr√©er un nom de fichier unique avec horodatage
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{timestamp}_{file_hash[:12]}.png"
    filepath = TEMP_DIR / filename
    
    # Sauvegarder le fichier
    with open(filepath, 'wb') as f:
        f.write(file_bytes)
    
    # Si ce n'est pas un PNG, convertir
    if not uploaded_file.name.lower().endswith('.png'):
        from PIL import Image
        img = Image.open(filepath)
        if img.mode != 'RGBA':
            img = img.convert('RGBA')
        img.save(filepath, 'PNG')
    
    return filepath

# Ajout du dossier parent au path pour importer les modules du projet
sys.path.append(str(Path(__file__).parent))

from fluxgym_coach.image_enhancement import ImageEnhancer
from fluxgym_coach.metadata import MetadataExtractor

def main():
    st.set_page_config(
        page_title="Fluxgym-coach - Am√©lioration d'images",
        page_icon="üñºÔ∏è",
        layout="wide"
    )
    
    st.title("üñºÔ∏è Fluxgym-coach - Mode Un par un")
    st.write("""
    Chargez une image pour l'am√©liorer avec l'IA. 
    Vous pourrez g√©n√©rer une description, ajuster les param√®tres, et valider le r√©sultat.
    """)
    
    # Initialisation des mod√®les
    if 'enhancer' not in st.session_state:
        st.session_state.enhancer = ImageEnhancer()
        
    @st.cache_resource
    def load_image_captioning_model():
        """Charge le mod√®le de g√©n√©ration de l√©gende d'image."""
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        try:
            # D√©sactiver les messages de warning de chargement
            import logging
            logging.getLogger("transformers").setLevel(logging.ERROR)
            
            # Utilisation d'un mod√®le BLIP standard plus stable
            model_name = "Salesforce/blip-image-captioning-base"
            
            # Charger le processeur et le mod√®le
            processor = AutoProcessor.from_pretrained(model_name)
            model = BlipForConditionalGeneration.from_pretrained(model_name).to(device)
            
            # D√©sactiver le mode √©valuation
            model.eval()
            
            return processor, model, device
            
        except Exception as e:
            st.error(f"Erreur lors du chargement du mod√®le : {str(e)}")
            return None, None, None
            
    def generate_detailed_description(image, processor, model, device):
        """G√©n√®re une description d√©taill√©e avec des attributs visuels pr√©cis."""
        try:
            # Configuration de la g√©n√©ration
            generation_kwargs = {
                "max_length": 300,
                "num_beams": 5,
                "no_repeat_ngram_size": 2,
                "early_stopping": True,
                "do_sample": True,
                "top_p": 0.95,
                "temperature": 0.7
            }
            
            # 1. Description des personnes
            person_prompt = "D√©cris en d√©tail chaque personne visible dans l'image. Pour chaque personne, indique :\n" \
                          "- √Çge approximatif et genre\n" \
                          "- Couleur et style de cheveux\n" \
                          "- Couleur des yeux et expression du visage\n" \
                          "- Posture et gestes\n" \
                          "- √âmotion et attitude"
            
            inputs = processor(image, text=person_prompt, return_tensors="pt").to(device)
            generated_ids = model.generate(**inputs, **generation_kwargs)
            person_description = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # 2. Description des v√™tements et accessoires
            clothes_prompt = "D√©cris les v√™tements et accessoires de chaque personne :\n" \
                           "- Type de tenue (d√©contract√©e, formelle, sportive, etc.)\n" \
                           "- Couleurs et motifs\n" \
                           "- Accessoires (lunettes, bijoux, chapeaux, etc.)\n" \
                           "- √âtat et style g√©n√©ral"
            
            inputs = processor(image, text=clothes_prompt, return_tensors="pt").to(device)
            generated_ids = model.generate(**inputs, **generation_kwargs)
            clothes_description = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # 3. Description de l'environnement
            env_prompt = "D√©cris l'environnement de l'image :\n" \
                       "- Type de lieu (int√©rieur/ext√©rieur, urbain/naturel, etc.)\n" \
                       "- √âl√©ments du d√©cor\n" \
                       "- √âclairage et ambiance\n" \
                       "- Conditions m√©t√©orologiques si visibles"
            
            inputs = processor(image, text=env_prompt, return_tensors="pt").to(device)
            generated_ids = model.generate(**inputs, **generation_kwargs)
            env_description = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # 4. Description des actions
            action_prompt = "D√©cris ce qui se passe dans l'image :\n" \
                          "- Actions des personnes\n" \
                          "- Interactions entre les personnes ou avec des objets\n" \
                          "- Activit√© principale\n" \
                          "- Mouvement ou dynamique de la sc√®ne"
            
            inputs = processor(image, text=action_prompt, return_tensors="pt").to(device)
            generated_ids = model.generate(**inputs, **generation_kwargs)
            action_description = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # 5. Analyse visuelle
            visual_prompt = "Analyse les aspects visuels de l'image :\n" \
                          "- Couleurs dominantes\n" \
                          "- Composition et cadrage\n" \
                          "- √âl√©ments qui attirent l'attention\n" \
                          "- Ambiance g√©n√©rale"
            
            inputs = processor(image, text=visual_prompt, return_tensors="pt").to(device)
            generated_ids = model.generate(**inputs, **generation_kwargs)
            visual_description = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # G√©n√©ration des tags
            tags_prompt = "G√©n√®re une liste de mots-cl√©s d√©taill√©s pour cette image, en incluant :\n" \
                        "1. Types de personnes (ex: homme adulte, enfant, femme √¢g√©e)\n" \
                        "2. Couleurs dominantes\n" \
                        "3. √âmotions et expressions\n" \
                        "4. Actions et activit√©s\n" \
                        "5. √âl√©ments remarquables\n" \
                        "Format de r√©ponse : Liste les mots-cl√©s s√©par√©s par des virgules, sans num√©rotation."
            
            inputs = processor(image, text=tags_prompt, return_tensors="pt").to(device)
            generated_ids = model.generate(**inputs, **generation_kwargs)
            tags_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            # Nettoyage et formatage
            def clean_text(text):
                text = text.replace("Caption:", "").strip()
                if text and not text.endswith('.'):
                    text += "."
                return text[0].upper() + text[1:]  # Majuscule au d√©but
            
            # Construction de la description finale
            final_description = "\n\n".join([
                f"PERSONNES : {clean_text(person_description)}",
                f"V√äTEMENTS : {clean_text(clothes_description)}",
                f"ENVIRONNEMENT : {clean_text(env_description)}",
                f"ACTIONS : {clean_text(action_description)}",
                f"ANALYSE VISUELLE : {clean_text(visual_description)}"
            ])
            
            # Nettoyage et formatage des tags
            tags = [tag.strip() for tag in tags_text.split(",") if tag.strip()]
            tags = list(dict.fromkeys(tags))  # Supprimer les doublons
            tags = [tag[0].upper() + tag[1:] for tag in tags]  # Majuscule au d√©but
            
            # Format final
            result = f"{final_description}\n\n√âL√âMENTS CL√âS: {', '.join(tags[:15])}"
            
            return result
            
        except Exception as e:
            print(f"Erreur lors de la g√©n√©ration de la description: {str(e)}")
            return "Impossible de g√©n√©rer une description pour cette image."
    
    # Nettoyer le dossier temporaire au d√©marrage
    if 'temp_cleaned' not in st.session_state:
        clean_temp_dir()
        st.session_state.temp_cleaned = True
    
    # Section de t√©l√©chargement d'image
    st.header("1. Chargement de l'image")
    uploaded_file = st.file_uploader("T√©l√©chargez une image", type=["jpg", "jpeg", "png"])
    
    if uploaded_file is not None:
        # Traiter l'image (hash, conversion PNG, sauvegarde temporaire)
        with st.spinner("Traitement de l'image..."):
            try:
                # Traitement de l'image
                processed_path = process_uploaded_image(uploaded_file)
                
                # Charger l'image trait√©e
                image = Image.open(processed_path)
                
                # Afficher les informations de traitement
                st.success("‚úÖ Image trait√©e avec succ√®s")
                st.info(f"Nom du fichier : {processed_path.name}")
                st.info(f"Taille : {os.path.getsize(processed_path) / 1024:.1f} Ko")
                
                # Afficher l'image
                st.image(image, caption="Image trait√©e et pr√™te pour l'analyse", use_container_width=True)
                
                # Section de description
                st.header("2. Description de l'image")
                
                # Charger le mod√®le de g√©n√©ration de description
                with st.spinner("Chargement du mod√®le de description..."):
                    processor, model, device = load_image_captioning_model()
                
                if processor is not None and model is not None:
                    # G√©n√©rer la description
                    with st.spinner("Analyse de l'image et g√©n√©ration de la description..."):
                        description = generate_detailed_description(image, processor, model, device)
                    
                    # Afficher la description
                    st.text_area("Description g√©n√©r√©e", description, height=200)
                    
                    # Section de param√®tres d'am√©lioration
                    st.header("3. Param√®tres d'am√©lioration")
                    
                    # Options d'am√©lioration
                    enhancement_options = {
                        "upscale": st.checkbox("Augmenter la r√©solution", value=True),
                        "denoise": st.checkbox("R√©duction du bruit", value=True),
                        "contrast": st.checkbox("Am√©lioration du contraste", value=True)
                    }
                    
                    # Bouton de traitement
                    if st.button("Traiter l'image"):
                        with st.spinner("Traitement en cours..."):
                            # Ici, vous ajouterez le code pour traiter l'image
                            # avec les options s√©lectionn√©es
                            processed_image = image  # Pour l'instant, on retourne l'image originale
                            
                            # Afficher l'image trait√©e
                            st.image(processed_image, caption="Image trait√©e", use_container_width=True)
                            
                            # Bouton de t√©l√©chargement
                            buffered = io.BytesIO()
                            processed_image.save(buffered, format="PNG")
                            st.download_button(
                                label="T√©l√©charger l'image trait√©e",
                                data=buffered.getvalue(),
                                file_name=f"{processed_path.stem}_traitee.png",
                                mime="image/png"
                            )
                            
                            # Nettoyer le dossier temporaire apr√®s le traitement
                            clean_temp_dir()
                            
            except Exception as e:
                st.error(f"Erreur lors du traitement de l'image : {str(e)}")
                st.exception(e)
                
    # Boutons de validation
    if 'processed_image' in st.session_state:
        st.subheader("Validation")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("‚úÖ Valider et continuer"):
                # Logique pour passer √† l'image suivante
                st.success("Image valid√©e avec succ√®s !")
                # Nettoyage apr√®s validation
                if 'processed_image' in st.session_state:
                    del st.session_state.processed_image
                
        with col2:
            if st.button("üîÑ Recommencer"):
                # R√©initialiser l'√©tat
                if 'processed_image' in st.session_state:
                    del st.session_state.processed_image
                st.rerun()

if __name__ == "__main__":
    main()
